_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 128, 28, 28)       2176      
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 128, 28, 28)       262272    
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 128, 14, 14)       0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 128, 14, 14)       0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 256, 14, 14)       524544    
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 256, 14, 14)       1048832   
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 256, 7, 7)         0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 256, 7, 7)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 12544)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               6423040   
_________________________________________________________________
dropout_3 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 10)                5130      
=================================================================
Total params: 8,265,994
Trainable params: 8,265,994
Non-trainable params: 0
_________________________________________________________________

* Create a new model. *

Train on 48000 samples, validate on 12000 samples
Epoch 1/30
48000/48000 [==============================] - 151s - loss: 0.4963 - acc: 0.8216 - val_loss: 0.3180 - val_acc: 0.8837
Epoch 2/30
48000/48000 [==============================] - 149s - loss: 0.3508 - acc: 0.8712 - val_loss: 0.2970 - val_acc: 0.8892
Epoch 3/30
48000/48000 [==============================] - 148s - loss: 0.3263 - acc: 0.8806 - val_loss: 0.2715 - val_acc: 0.8958
Epoch 4/30
48000/48000 [==============================] - 148s - loss: 0.3142 - acc: 0.8860 - val_loss: 0.2704 - val_acc: 0.9028
Epoch 5/30
48000/48000 [==============================] - 148s - loss: 0.3061 - acc: 0.8896 - val_loss: 0.2584 - val_acc: 0.9008
Epoch 6/30
48000/48000 [==============================] - 148s - loss: 0.2954 - acc: 0.8926 - val_loss: 0.2518 - val_acc: 0.9067
Epoch 7/30
48000/48000 [==============================] - 148s - loss: 0.2951 - acc: 0.8926 - val_loss: 0.2588 - val_acc: 0.9008
Epoch 8/30
48000/48000 [==============================] - 148s - loss: 0.2933 - acc: 0.8935 - val_loss: 0.2638 - val_acc: 0.9056
Epoch 9/30
48000/48000 [==============================] - 148s - loss: 0.2908 - acc: 0.8946 - val_loss: 0.2419 - val_acc: 0.9091
Epoch 10/30
48000/48000 [==============================] - 147s - loss: 0.2917 - acc: 0.8935 - val_loss: 0.2469 - val_acc: 0.9068
Epoch 11/30
48000/48000 [==============================] - 147s - loss: 0.2866 - acc: 0.8961 - val_loss: 0.2562 - val_acc: 0.9062
Epoch 12/30
48000/48000 [==============================] - 147s - loss: 0.2888 - acc: 0.8974 - val_loss: 0.2443 - val_acc: 0.9114
Epoch 13/30
48000/48000 [==============================] - 147s - loss: 0.2929 - acc: 0.8951 - val_loss: 0.2368 - val_acc: 0.9139
Epoch 14/30
48000/48000 [==============================] - 148s - loss: 0.2789 - acc: 0.8988 - val_loss: 0.2459 - val_acc: 0.9083
Epoch 15/30
48000/48000 [==============================] - 147s - loss: 0.2867 - acc: 0.8970 - val_loss: 0.2452 - val_acc: 0.9119
Epoch 16/30
48000/48000 [==============================] - 147s - loss: 0.2827 - acc: 0.8990 - val_loss: 0.2356 - val_acc: 0.9127
Epoch 17/30
48000/48000 [==============================] - 147s - loss: 0.2855 - acc: 0.8960 - val_loss: 0.2634 - val_acc: 0.9032
Epoch 18/30
48000/48000 [==============================] - 147s - loss: 0.2883 - acc: 0.8985 - val_loss: 0.2373 - val_acc: 0.9147
Epoch 19/30
48000/48000 [==============================] - 147s - loss: 0.2824 - acc: 0.8997 - val_loss: 0.2483 - val_acc: 0.9109
Epoch 20/30
48000/48000 [==============================] - 147s - loss: 0.2870 - acc: 0.8963 - val_loss: 0.2323 - val_acc: 0.9153
Epoch 21/30
48000/48000 [==============================] - 147s - loss: 0.2922 - acc: 0.8955 - val_loss: 0.2398 - val_acc: 0.9105
Epoch 22/30
48000/48000 [==============================] - 147s - loss: 0.2901 - acc: 0.8953 - val_loss: 0.2423 - val_acc: 0.9083
Epoch 23/30
48000/48000 [==============================] - 147s - loss: 0.2909 - acc: 0.8953 - val_loss: 0.2363 - val_acc: 0.9148
Epoch 24/30
48000/48000 [==============================] - 147s - loss: 0.2810 - acc: 0.8993 - val_loss: 0.2510 - val_acc: 0.9058
Epoch 25/30
48000/48000 [==============================] - 147s - loss: 0.2868 - acc: 0.8981 - val_loss: 0.2416 - val_acc: 0.9088
Epoch 26/30
48000/48000 [==============================] - 147s - loss: 0.2800 - acc: 0.8975 - val_loss: 0.2389 - val_acc: 0.9126
Epoch 27/30
48000/48000 [==============================] - 147s - loss: 0.2868 - acc: 0.8965 - val_loss: 0.2407 - val_acc: 0.9113
Epoch 28/30
48000/48000 [==============================] - 147s - loss: 0.2929 - acc: 0.8956 - val_loss: 0.2512 - val_acc: 0.9097
Epoch 29/30
48000/48000 [==============================] - 147s - loss: 0.2914 - acc: 0.8950 - val_loss: 0.2572 - val_acc: 0.9003
Epoch 30/30
48000/48000 [==============================] - 147s - loss: 0.2866 - acc: 0.8971 - val_loss: 0.2372 - val_acc: 0.9107
